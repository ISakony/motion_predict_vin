{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import chainer\n",
    "from chainer import cuda, serializers, functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Chain, Variable, cuda, optimizer, optimizers, serializers\n",
    "import pickle\n",
    "import os\n",
    "from chainer.links import caffe\n",
    "\n",
    "def load_list(path):\n",
    "    tuples = []\n",
    "    for line in open(path):\n",
    "        pair = line.replace(\"\\n\",\"\")#.strip().split()\n",
    "        #print(pair)\n",
    "        tuples.append(pair)\n",
    "    return tuples\n",
    "\n",
    "#print(load_list(\"train.txt\"))\n",
    "\n",
    "def pos_feat_diff(feat,b_po,af_po):#Position feature difference\n",
    "    batch,ch,w,h = feat.data.shape\n",
    "    b_po = F.broadcast_to(b_po,(batch,ch,w,h)) #(1,1,46,46) to (1,128,46,46)\n",
    "    af_po = F.broadcast_to(af_po,(batch,ch,w,h))\n",
    "    feat = feat - b_po + af_po\n",
    "    return feat\n",
    "\n",
    "def pos_feat_sum(po):#Position Features Summary  shape (19,320,320)\n",
    "    po = F.sum(po,axis=0,keepdims=True)\n",
    "    po = F.resize_images(F.expand_dims(po,axis=0),(46,46))\n",
    "    return po #shape (1,1,46,46)\n",
    "\n",
    "class MEVIN(chainer.Chain):\n",
    "    insize = 320\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MEVIN, self).__init__(\n",
    "            mlp1_1 = chainer.ChainList(\n",
    "                *[L.Linear(None,46*46)\n",
    "                  for i in range(19)]),\n",
    "            mlp1_2 = chainer.ChainList(\n",
    "                *[L.Linear(None,4)\n",
    "                  for i in range(19)]),\n",
    "            mlp2_1 = chainer.ChainList(\n",
    "                *[L.Linear(None,4)\n",
    "                  for i in range(19*19)]),\n",
    "            mlp2_2 = chainer.ChainList(\n",
    "                *[L.Linear(None,2)\n",
    "                  for i in range(19*19)]),\n",
    "            mlp3_1 = L.Linear(None,46*46),\n",
    "            mlp3_2 = L.Linear(None,46*46),\n",
    "            conv1 = L.Convolution2D(in_channels=1, out_channels=19, ksize=3, stride=1, pad=1),\n",
    "            conv1_a = L.Convolution2D(in_channels=19, out_channels=19, ksize=1, stride=1),\n",
    "            conv1_b = L.Convolution2D(in_channels=19, out_channels=19, ksize=1, stride=1),\n",
    "            conv2 = L.Convolution2D(in_channels=19, out_channels=19, ksize=3, stride=1, pad=1),\n",
    "            conv2_a = L.Convolution2D(in_channels=19, out_channels=19, ksize=1, stride=1),\n",
    "            conv2_b = L.Convolution2D(in_channels=19, out_channels=19, ksize=1, stride=1),\n",
    "            conv3 = L.Convolution2D(in_channels=19, out_channels=19, ksize=3, stride=1, pad=1),\n",
    "            conv3_a = L.Convolution2D(in_channels=19, out_channels=19, ksize=1, stride=1),\n",
    "            conv3_b = L.Convolution2D(in_channels=19, out_channels=19, ksize=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x1,x2,x3,x4):#Pred = [x1,x2,x3,x4]\n",
    "        #print(x1.shape)\n",
    "        h1 = self.Mix_pofe(x1,x2,x3)\n",
    "        h2 = self.Mix_pofe(x2,x3,x4)\n",
    "        \n",
    "        #print(len(h1))\n",
    "        h1 = self.Interaction(h1)\n",
    "        h2 = self.Interaction(h2)\n",
    "        \n",
    "        h = self.Aggregator(h1,h2) # (1,1,42,42)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "    def Mix_pofe(self,b_x,af_x,aff_x):        \n",
    "        b_x_list = F.split_axis(b_x, 19, axis=0) #[(1,320,320),......]\n",
    "        af_x_list = F.split_axis(af_x, 19, axis=0) #[(1,320,320),......]\n",
    "        aff_x_list = F.split_axis(aff_x, 19, axis=0)\n",
    "        \n",
    "        afb_x = []\n",
    "        for i in range(len(b_x_list)):\n",
    "            aff_af = F.concat((af_x_list[i],aff_x_list[i]),axis=1)\n",
    "            afb_x.append(F.expand_dims(F.concat((b_x_list[i],aff_af),axis=1),axis=0)) #(1,3,320,320)\n",
    "            \n",
    "        return afb_x\n",
    "    \n",
    "    def Interaction(self,afb_x): \n",
    "        F_mlp = []\n",
    "        for i in range(len(afb_x)):\n",
    "            img = F.resize_images(afb_x[i],(46,46))\n",
    "            pred = F.leaky_relu(self.mlp1_1[i](img))#(1,3,46,46) >> (1,46*46)\n",
    "            pred = F.leaky_relu(self.mlp1_2[i](pred))#(1,24*24) >> (1,4)\n",
    "            F_mlp.append(pred)\n",
    "        \n",
    "        self_inter = []\n",
    "        inter = 0\n",
    "        for i in range(len(F_mlp)):\n",
    "            for ii in range(len(F_mlp)):\n",
    "                #print(i)\n",
    "                #print(ii)\n",
    "                if i == ii:\n",
    "                    h = F.concat((F_mlp[i],F_mlp[ii]),axis=1)\n",
    "                    h = F.leaky_relu(self.mlp2_1[i * 19 + ii](h)) #(1,4)\n",
    "                    #print(\"chack\",h)\n",
    "                    self_inter.append(self.mlp2_2[i * 19 + ii](h)) #(1,2)\n",
    "                elif i != ii:\n",
    "                    h = F.concat((F_mlp[i],F_mlp[ii]),axis=1)\n",
    "                    h = F.leaky_relu(self.mlp2_1[i * 19 + ii](h)) #(1,4)\n",
    "                    #print(\"test\",h)\n",
    "                    inter += F.leaky_relu(self.mlp2_2[i * 19 + ii](h))#(1,2)\n",
    "                                          \n",
    "        for iii in range(len(self_inter)):\n",
    "            self_inter[iii] += inter\n",
    "                                          \n",
    "        self_inter_add_inter = F.stack(self_inter, axis=1)\n",
    "        \n",
    "        return self_inter_add_inter #(1,19*2)\n",
    "                    \n",
    "    def Aggregator(self,si_add1,si_add2):\n",
    "        h = F.concat((si_add1,si_add2),axis=1)#(19,2*2)\n",
    "        h = F.leaky_relu(self.mlp3_1(h)) #(1,46*46)\n",
    "        h = F.leaky_relu(self.mlp3_2(h)) #(1,46*46)\n",
    "        h = F.reshape(h,(1,1,46,46))\n",
    "        \n",
    "        h2 = F.leaky_relu(self.conv1(h))\n",
    "        h2 = F.leaky_relu(self.conv1_a(h2))    \n",
    "        h2 = F.leaky_relu(self.conv1_b(h2))\n",
    "        h3 = F.leaky_relu(self.conv2(h2))\n",
    "        h3 = F.leaky_relu(self.conv2_a(h3))    \n",
    "        h3 = F.leaky_relu(self.conv2_b(h3))    \n",
    "        h4 = F.leaky_relu(self.conv2(h3 + h2))\n",
    "        h4 = F.leaky_relu(self.conv2_a(h4))    \n",
    "        h4 = F.leaky_relu(self.conv2_b(h4)) \n",
    "        \n",
    "        return h,h4\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "model = MEVIN()\n",
    "serializers.load_npz(\"mevin/model600.npz\", model)\n",
    "\n",
    "# GPUのセット\n",
    "FLAG_GPU = False # GPUを使用するかどうか\n",
    "if FLAG_GPU: # numpyかcuda.cupyか\n",
    "    xp = cuda.cupy\n",
    "    cuda.get_device(0).use()\n",
    "    model.to_gpu()\n",
    "else:\n",
    "    xp = np\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測スタートフレーム設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "#スタートフレーム\n",
    "start_f = 30\n",
    "\n",
    "#予測フレーム数\n",
    "n_frame = 4\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_42157.jpg\n",
      "1_42158.jpg\n",
      "1_42159.jpg\n",
      "1_42160.jpg\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "train_list = load_list(\"train.txt\")\n",
    "\n",
    "Preds = []\n",
    "for i in range(4): \n",
    "    ii = i + start_f\n",
    "    with open('use_data_analis/'+ train_list[ii] +'_po.pickle', mode='rb') as f:\n",
    "        print(train_list[ii])\n",
    "        po = pickle.load(f)\n",
    "        if FLAG_GPU:\n",
    "            po = cuda.to_gpu(po)\n",
    "    Preds.append(po)\n",
    "\n",
    "for i in range(n_frame):\n",
    "    print(i)\n",
    "    with chainer.using_config('train', 'False'):\n",
    "        result, pre_po = model(Preds[0],Preds[1],Preds[2],Preds[3])\n",
    "\n",
    "    Preds.pop(0)\n",
    "    Preds.append(gaussian_filter(F.resize_images(pre_po,(320,320)).data[0], sigma=2.5))\n",
    "            \n",
    "    if FLAG_GPU:\n",
    "        reimg = cuda.to_cpu(F.resize_images(pos_feat_sum(pre_po[0]),(210,210)).data)\n",
    "\n",
    "    else:\n",
    "        reimg = F.resize_images(pos_feat_sum(pre_po[0]),(210,210)).data\n",
    "                    \n",
    "    cv2.imwrite('result_fillter/reimg'+str(i)+'.png', (reimg.transpose(0,2,3,1)[0])*256)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
